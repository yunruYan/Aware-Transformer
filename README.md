# Aware-Transformer : A novel pure Transformer-based model for Remote Sensing Image Captioning
# main.py 
# model: encoder-decoder architecture 
# mssydney: processed data
# AwareT_Sydney: execute folder


#Note: Python 3.6 is required to run our code.
## Requirements
- cuda>10
- pytorch 1.10.0
- Matplotlib
- PIL
- NLTK
- Sydney captions, UCM captions and RSICD datasets from "https://github.com/201528014227051/RSICD_optimal".
-NWPU-Captions dataset from:"https://github.com/HaiyanHuang98/NWPU-Captions"

## Acknowledgements
This repository is based on [JDAI-CV/image-captioning](https://github.com/JDAI-CV/image-captioning), 
[ruotianluo/self-critical.pytorch](https://github.com/ruotianluo/self-critical.pytorch) and 
[microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer).
